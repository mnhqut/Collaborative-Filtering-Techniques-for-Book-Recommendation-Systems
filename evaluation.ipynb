{"cells":[{"cell_type":"markdown","source":["# Import"],"metadata":{"id":"yBntzVltWzYb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NsJ5SYoOwx1I"},"outputs":[],"source":["import sys\n","from recommenders.utils.timer import Timer\n","from recommenders.datasets.python_splitters import python_stratified_split\n","from recommenders.evaluation.python_evaluation import (\n","    rmse,\n","    mae,\n","    rsquared,\n","    exp_var,\n","    map_at_k,\n","    ndcg_at_k,\n","    precision_at_k,\n","    recall_at_k,\n","    get_top_k_items,\n","    serendipity,\n","    diversity,\n","    novelty,\n",")"]},{"cell_type":"code","source":["from surprise import Dataset, SVDpp, SVD, NormalPredictor, accuracy,  Reader\n","from surprise.model_selection import train_test_split\n","from surprise.model_selection import cross_validate\n","from recommenders.models.surprise.surprise_utils import (\n","    predict,\n","    # compute_ranking_predictions,\n",")\n","import surprise\n","\n","import os\n","import sys\n","import cornac\n","import pandas as pd\n","# from recommenders.models.cornac.cornac_utils import predict_ranking\n","from recommenders.utils.timer import Timer\n","\n","\n","import numpy as np\n","import tensorflow as tf\n","tf.get_logger().setLevel('ERROR') # only show error messages\n","from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN\n","from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n","from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n","\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n"],"metadata":{"id":"2VpvfPTOSVmS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_ranking_predictions(\n","    algo,\n","    users,\n","    items,\n","    data,\n","    usercol='userID',\n","    itemcol='itemID',\n","    predcol='prediction',\n","    remove_seen=False,\n","):\n","\n","    preds_lst = []\n","    # users = data[usercol].unique()\n","    # items = data[itemcol].unique()\n","\n","    for user in users:\n","      for item in items:\n","          preds_lst.append([user, item, algo.predict(user, item).est])\n","\n","    all_predictions = pd.DataFrame(data=preds_lst, columns=[usercol, itemcol, predcol])\n","\n","    if remove_seen:\n","        tempdf = pd.concat(\n","            [\n","                data[[usercol, itemcol]],\n","                pd.DataFrame(\n","                    data=np.ones(data.shape[0]), columns=[\"dummycol\"], index=data.index\n","                ),\n","            ],\n","            axis=1,\n","        )\n","        merged = pd.merge(tempdf, all_predictions, on=[usercol, itemcol], how=\"outer\")\n","        return merged[merged[\"dummycol\"].isnull()].drop(\"dummycol\", axis=1)\n","    else:\n","        return all_predictions"],"metadata":{"id":"mMpTEWvwlejl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_ranking(\n","    model,\n","    users,\n","    items,\n","    data,\n","    usercol='userID',\n","    itemcol='itemID',\n","    predcol='prediction',\n","    remove_seen=False,\n","):\n","\n","    users_list, items_list, preds_list = [], [], []\n","\n","    for uid, user_idx in users:\n","        user = [uid] * len(items)\n","        users_list.extend(user)\n","        items_list.extend(items)\n","        preds_list.extend(model.score(user_idx).tolist())\n","\n","    all_predictions = pd.DataFrame(\n","        data={usercol: users_list, itemcol: items_list, predcol: preds_list}\n","    )\n","\n","    if remove_seen:\n","        tempdf = pd.concat(\n","            [\n","                data[[usercol, itemcol]],\n","                pd.DataFrame(\n","                    data=np.ones(data.shape[0]), columns=[\"dummycol\"], index=data.index\n","                ),\n","            ],\n","            axis=1,\n","        )\n","        merged = pd.merge(tempdf, all_predictions, on=[usercol, itemcol], how=\"outer\")\n","        return merged[merged[\"dummycol\"].isnull()].drop(\"dummycol\", axis=1)\n","    else:\n","        return all_predictions"],"metadata":{"id":"QIgw6xOJloXC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Parameters"],"metadata":{"id":"lID2twMHW8mY"}},{"cell_type":"code","source":["train_file_path = ''\n","test_file_path = ''\n","\n","TOP_K = 10\n","threshold =20\n","relevancy_method='by_threshold'\n","\n","implicit_threshold = 5\n","\n","#for surprise models\n","reader = Reader(rating_scale=(1, 10))\n","\n","# Use the SVD++ algorithm\n","svdpp = SVDpp(n_factors=50,n_epochs=40,lr_all=0.007, reg_all=0.02, cache_ratings=True, verbose = True)\n","\n","# Use the SVD algorithm\n","svd = SVD(n_factors=50,n_epochs=40,lr_all=0.005, reg_all=0.02,verbose=True)\n","\n","# Use the random algorithm\n","random = NormalPredictor()\n","\n","#bpr\n","bpr = cornac.models.BPR(\n","    k=200,\n","    max_iter=100,\n","    learning_rate=0.01,\n","    lambda_reg=0.001#,\n","    # verbose=True\n",")\n","\n","#lightgcn\n","hparams = prepare_hparams(model_type = \"lightgcn\",\n","                          embed_size = 40,\n","                          n_layers=7,\n","                          batch_size=1024,\n","                          epochs=40,\n","                          decay = 0.0001,\n","                          learning_rate=0.003,\n","                          eval_epoch=5,\n","                          top_k=TOP_K,\n","                          save_model= False,\n","                          save_epoch=100,\n","                          metrics = [\"recall\", \"ndcg\", \"precision\", \"map\"],\n","                          MODEL_DIR = './tests/resources/deeprec/lightgcn/model/lightgcn_model/'\n","                         )"],"metadata":{"id":"kLnIaCXVW--d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load data"],"metadata":{"id":"At0J4x5dIj9B"}},{"cell_type":"code","source":["import pandas as pd\n","# Load the CSV file\n","train = pd.read_csv(train_file_path, sep=',')\n","test = pd.read_csv(test_file_path, sep=',')\n","\n","print(train.info())"],"metadata":{"id":"Tvl4-ZOP-4Sh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test.info())"],"metadata":{"id":"NCzLd3KEm8um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["impli_train = train[train['rating'] >= implicit_threshold]"],"metadata":{"id":"hMcaIfDqpbdR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  SVD+\n","\n","https://github.com/NicolasHug/Surprise/issues/123"],"metadata":{"id":"9Ph0J5iACtQS"}},{"cell_type":"code","source":["trainset = Dataset.load_from_df(train[['userID', 'itemID', 'rating']], reader).build_full_trainset()\n","\n","# Train the algorithm on the trainset\n","svdpp.fit(trainset)\n","\n","predictions = predict(svdpp, test, usercol=\"userID\", itemcol=\"itemID\")\n"],"metadata":{"id":"oAss_JBgSgD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_rmse = rmse(test, predictions)\n","eval_mae = mae(test, predictions)\n","eval_rsquared = rsquared(test, predictions)\n","eval_exp_var = exp_var(test, predictions)\n","\n","print(\n","    \"RMSE:\\t\\t%f\" % eval_rmse,\n","    \"MAE:\\t\\t%f\" % eval_mae,\n","    \"rsquared:\\t%f\" % eval_rsquared,\n","    \"exp var:\\t%f\" % eval_exp_var,\n","    sep=\"\\n\",\n",")\n","\n","print(\"----\")\n","\n","# Get the unique userIDs\n","unique_user_ids = train['userID'].unique()\n","\n","# Split into chunks of 100 unique userIDs\n","user_chunks = [unique_user_ids[i:i + 200] for i in range(0, len(unique_user_ids), 200)]\n","\n","# Create a list to store the resulting DataFrames\n","train_chunks = []\n","test_chunks = []\n","\n","# Loop through each chunk and filter the DataFrame\n","for chunk in user_chunks:\n","    train_chunks.append(train[train['userID'].isin(chunk)])\n","    test_chunks.append(test[test['userID'].isin(chunk)])\n","\n","\n","eval_map_chunk = []\n","eval_ndcg_chunk = []\n","eval_precision_chunk = []\n","eval_recall_chunk = []\n","\n","eval_serendipity_chunk = []\n","eval_diversity_chunk = []\n","eval_novelty_chunk = []\n","\n","n = len(user_chunks)\n","items = train.itemID.unique()\n","i = 0\n","\n","for i in range(0,n):\n","    users = user_chunks[i]\n","    user_data = train_chunks[i]\n","    test_data = test_chunks[i]\n","\n","    all_predictions_chunk = compute_ranking_predictions(\n","        svdpp, users, items, user_data, usercol=\"userID\", itemcol=\"itemID\", remove_seen=True\n","    )\n","\n","\n","    eval_map_chunk.append(map_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                    relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_ndcg_chunk.append(ndcg_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","\n","    eval_precision_chunk.append(precision_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_recall_chunk.append(recall_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                          relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","\n","    rec_df = get_top_k_items(all_predictions_chunk, col_rating='prediction', k=TOP_K)\n","    eval_serendipity_chunk.append(serendipity(user_data, rec_df))\n","    eval_diversity_chunk.append(diversity(user_data, rec_df))\n","    eval_novelty_chunk.append(novelty(user_data, rec_df))\n","\n","    i += 1\n","    print(i)\n","\n","# Aggregate results\n","eval_map = np.mean(eval_map_chunk)\n","eval_ndcg = np.mean(eval_ndcg_chunk)\n","eval_precision = np.mean(eval_precision_chunk)\n","eval_recall = np.mean(eval_recall_chunk)\n","\n","eval_serendipity = np.mean(eval_serendipity_chunk)\n","eval_diversity = np.mean(eval_diversity_chunk)\n","eval_novelty = np.mean(eval_novelty_chunk)\n","\n","# Print results\n","print(\n","    \"MAP:\\t\\t%f\" % eval_map,\n","    \"NDCG:\\t\\t%f\" % eval_ndcg,\n","    \"Precision@K:\\t%f\" % eval_precision,\n","    \"Recall@K:\\t%f\" % eval_recall,\n","    sep=\"\\n\",\n",")\n","\n","print(\"----\")\n","\n","print(\n","    \"Diversity:\\t\\t%f\" % eval_diversity,\n","    \"Novelty:\\t\\t%f\" % eval_novelty,\n","    \"Serendipity:\\t\\t%f\" % eval_serendipity,\n","    sep=\"\\n\",\n",")"],"metadata":{"id":"VIWUDXH3mfHT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SVD"],"metadata":{"id":"f3F-f2a4emfV"}},{"cell_type":"code","source":["trainset = Dataset.load_from_df(train[['userID', 'itemID', 'rating']], reader).build_full_trainset()\n","\n","# Train the algorithm on the trainset\n","svd.fit(trainset)\n","\n","predictions = predict(svd, test, usercol=\"userID\", itemcol=\"itemID\")\n","\n"],"metadata":{"id":"d3_83Rktez6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_rmse = rmse(test, predictions)\n","eval_mae = mae(test, predictions)\n","eval_rsquared = rsquared(test, predictions)\n","eval_exp_var = exp_var(test, predictions)\n","\n","print(\n","    \"RMSE:\\t\\t%f\" % eval_rmse,\n","    \"MAE:\\t\\t%f\" % eval_mae,\n","    \"rsquared:\\t%f\" % eval_rsquared,\n","    \"exp var:\\t%f\" % eval_exp_var,\n","    sep=\"\\n\",\n",")\n","\n","print(\"----\")\n","\n","# Get the unique userIDs\n","unique_user_ids = train['userID'].unique()\n","\n","# Split into chunks of 100 unique userIDs\n","user_chunks = [unique_user_ids[i:i + 200] for i in range(0, len(unique_user_ids), 200)]\n","\n","# Create a list to store the resulting DataFrames\n","train_chunks = []\n","test_chunks = []\n","\n","# Loop through each chunk and filter the DataFrame\n","for chunk in user_chunks:\n","    train_chunks.append(train[train['userID'].isin(chunk)])\n","    test_chunks.append(test[test['userID'].isin(chunk)])\n","\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","eval_map_chunk = []\n","eval_ndcg_chunk = []\n","eval_precision_chunk = []\n","eval_recall_chunk = []\n","\n","eval_serendipity_chunk = []\n","eval_diversity_chunk = []\n","eval_novelty_chunk = []\n","\n","n = len(user_chunks)\n","items = train.itemID.unique()\n","i = 0\n","\n","for i in range(0,n):\n","    users = user_chunks[i]\n","    user_data = train_chunks[i]\n","    test_data = test_chunks[i]\n","\n","    all_predictions_chunk = compute_ranking_predictions(\n","        svd, users, items, user_data, usercol=\"userID\", itemcol=\"itemID\", remove_seen=True\n","    )\n","\n","\n","    eval_map_chunk.append(map_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                    relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_ndcg_chunk.append(ndcg_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","\n","    eval_precision_chunk.append(precision_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_recall_chunk.append(recall_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                          relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","\n","    rec_df = get_top_k_items(all_predictions_chunk, col_rating='prediction', k=TOP_K)\n","    eval_serendipity_chunk.append(serendipity(user_data, rec_df))\n","    eval_diversity_chunk.append(diversity(user_data, rec_df))\n","    eval_novelty_chunk.append(novelty(user_data, rec_df))\n","\n","    i += 1\n","    print(i)\n","\n","# Aggregate results\n","eval_map = np.mean(eval_map_chunk)\n","eval_ndcg = np.mean(eval_ndcg_chunk)\n","eval_precision = np.mean(eval_precision_chunk)\n","eval_recall = np.mean(eval_recall_chunk)\n","\n","eval_serendipity = np.mean(eval_serendipity_chunk)\n","eval_diversity = np.mean(eval_diversity_chunk)\n","eval_novelty = np.mean(eval_novelty_chunk)\n","\n","# Print results\n","print(\n","    \"MAP:\\t\\t%f\" % eval_map,\n","    \"NDCG:\\t\\t%f\" % eval_ndcg,\n","    \"Precision@K:\\t%f\" % eval_precision,\n","    \"Recall@K:\\t%f\" % eval_recall,\n","    sep=\"\\n\",\n",")\n","\n","print(\"----\")\n","\n","print(\n","    \"Diversity:\\t\\t%f\" % eval_diversity,\n","    \"Novelty:\\t\\t%f\" % eval_novelty,\n","    \"Serendipity:\\t\\t%f\" % eval_serendipity,\n","    sep=\"\\n\",\n",")"],"metadata":{"id":"Aip3T5pQmuvI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random"],"metadata":{"id":"fuPGi-CAjNUt"}},{"cell_type":"code","source":["trainset = Dataset.load_from_df(train[['userID', 'itemID', 'rating']], reader).build_full_trainset()\n","\n","# Train the algorithm on the trainset\n","random.fit(trainset)\n","\n","predictions = predict(random, test, usercol='userID', itemcol=\"itemID\")\n","\n"],"metadata":{"id":"6nGdIeFDjaQW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724429542682,"user_tz":-420,"elapsed":517,"user":{"displayName":"16 1v1i1v8","userId":"07830316725673552979"}},"outputId":"aa9a11b3-cdfb-4f72-cb96-36354c78ee8f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<surprise.prediction_algorithms.random_pred.NormalPredictor at 0x7ff20d7c5030>"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["eval_rmse = rmse(test, predictions)\n","eval_mae = mae(test, predictions)\n","eval_rsquared = rsquared(test, predictions)\n","eval_exp_var = exp_var(test, predictions)\n","\n","print(\n","    \"RMSE:\\t\\t%f\" % eval_rmse,\n","    \"MAE:\\t\\t%f\" % eval_mae,\n","    \"rsquared:\\t%f\" % eval_rsquared,\n","    \"exp var:\\t%f\" % eval_exp_var,\n","    sep=\"\\n\",\n",")\n","\n","print(\"----\")\n","\n","# Get the unique userIDs\n","unique_user_ids = train['userID'].unique()\n","\n","# Split into chunks of 100 unique userIDs\n","user_chunks = [unique_user_ids[i:i + 200] for i in range(0, len(unique_user_ids), 200)]\n","\n","# Create a list to store the resulting DataFrames\n","train_chunks = []\n","test_chunks = []\n","\n","# Loop through each chunk and filter the DataFrame\n","for chunk in user_chunks:\n","    train_chunks.append(train[train['userID'].isin(chunk)])\n","    test_chunks.append(test[test['userID'].isin(chunk)])\n","\n","\n","eval_map_chunk = []\n","eval_ndcg_chunk = []\n","eval_precision_chunk = []\n","eval_recall_chunk = []\n","\n","eval_serendipity_chunk = []\n","eval_diversity_chunk = []\n","eval_novelty_chunk = []\n","\n","n = len(user_chunks)\n","items = train.itemID.unique()\n","i = 0\n","\n","for i in range(0,n):\n","    users = user_chunks[i]\n","    user_data = train_chunks[i]\n","    test_data = test_chunks[i]\n","\n","    all_predictions_chunk = compute_ranking_predictions(\n","        random, users, items, user_data, usercol=\"userID\", itemcol=\"itemID\", remove_seen=True\n","    )\n","\n","\n","    eval_map_chunk.append(map_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                    relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_ndcg_chunk.append(ndcg_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","\n","    eval_precision_chunk.append(precision_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_recall_chunk.append(recall_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                          relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","\n","    rec_df = get_top_k_items(all_predictions_chunk, col_rating='prediction', k=TOP_K)\n","    eval_serendipity_chunk.append(serendipity(user_data, rec_df))\n","    eval_diversity_chunk.append(diversity(user_data, rec_df))\n","    eval_novelty_chunk.append(novelty(user_data, rec_df))\n","\n","    i += 1\n","    print(i)\n","\n","# Aggregate results\n","eval_map = np.mean(eval_map_chunk)\n","eval_ndcg = np.mean(eval_ndcg_chunk)\n","eval_precision = np.mean(eval_precision_chunk)\n","eval_recall = np.mean(eval_recall_chunk)\n","\n","eval_serendipity = np.mean(eval_serendipity_chunk)\n","eval_diversity = np.mean(eval_diversity_chunk)\n","eval_novelty = np.mean(eval_novelty_chunk)\n","\n","# Print results\n","print(\n","    \"MAP:\\t\\t%f\" % eval_map,\n","    \"NDCG:\\t\\t%f\" % eval_ndcg,\n","    \"Precision@K:\\t%f\" % eval_precision,\n","    \"Recall@K:\\t%f\" % eval_recall,\n","    sep=\"\\n\",\n",")\n","\n","print(\"----\")\n","\n","print(\n","    \"Diversity:\\t\\t%f\" % eval_diversity,\n","    \"Novelty:\\t\\t%f\" % eval_novelty,\n","    \"Serendipity:\\t\\t%f\" % eval_serendipity,\n","    sep=\"\\n\",\n",")"],"metadata":{"id":"x7KZQwEGmvxa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BPR"],"metadata":{"id":"Maeiu3m_x4GC"}},{"cell_type":"code","source":["train_set = cornac.data.Dataset.from_uir(impli_train.itertuples(index=False))\n","\n","print('Number of users: {}'.format(train_set.num_users))\n","print('Number of items: {}'.format(train_set.num_items))\n","\n","with Timer() as t:\n","    bpr.fit(train_set)\n","print(\"Took {} seconds for training.\".format(t))"],"metadata":{"id":"SLVKvPdsyLnl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the unique userIDs\n","unique_user_ids = train['userID'].unique()\n","cor_internal_user_ids = [bpr.train_set.uid_map[uid] for uid in unique_user_ids]\n","in_ex_unique_users_ids = list(zip(unique_user_ids,cor_internal_user_ids))\n","\n","# Split into chunks of 100 unique userIDs\n","user_chunks = [in_ex_unique_users_ids[i:i + 200] for i in range(0, len(unique_user_ids), 200)]\n","chunks = [unique_user_ids[i:i + 200] for i in range(0, len(unique_user_ids), 200)]\n","\n","# Create a list to store the resulting DataFrames\n","train_chunks = []\n","test_chunks = []\n","\n","# Loop through each chunk and filter the DataFrame\n","for chunk in chunks:\n","    train_chunks.append(train[train['userID'].isin(chunk)])\n","    test_chunks.append(test[test['userID'].isin(chunk)])\n","\n","items = list(bpr.train_set.iid_map.keys())\n","\n","import numpy as np\n","\n","eval_map_chunk = []\n","eval_ndcg_chunk = []\n","eval_precision_chunk = []\n","eval_recall_chunk = []\n","\n","eval_serendipity_chunk = []\n","eval_diversity_chunk = []\n","eval_novelty_chunk = []\n","\n","n = len(user_chunks)\n","items = train.itemID.unique()\n","i = 0\n","\n","for i in range(0,n):\n","    users = user_chunks[i]\n","    # print(users)\n","    user_data = train_chunks[i]\n","    # print(user_data)\n","    test_data = test_chunks[i]\n","\n","    all_predictions_chunk = predict_ranking(\n","        bpr, users, items, user_data, usercol=\"userID\", itemcol=\"itemID\", remove_seen=True\n","    )\n","\n","    eval_map_chunk.append(map_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                    relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_ndcg_chunk.append(ndcg_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    # print(eval)\n","    eval_precision_chunk.append(precision_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_recall_chunk.append(recall_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                          relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","\n","    rec_df = get_top_k_items(all_predictions_chunk, col_rating='prediction', k=TOP_K)\n","    eval_serendipity_chunk.append(serendipity(user_data, rec_df))\n","    eval_diversity_chunk.append(diversity(user_data, rec_df))\n","    eval_novelty_chunk.append(novelty(user_data, rec_df))\n","\n","\n","    i += 1\n","    print(i)\n","\n","\n","eval_map = np.mean(eval_map_chunk)\n","eval_ndcg = np.mean(eval_ndcg_chunk)\n","eval_precision = np.mean(eval_precision_chunk)\n","eval_recall = np.mean(eval_recall_chunk)\n","\n","eval_serendipity = np.mean(eval_serendipity_chunk)\n","eval_diversity = np.mean(eval_diversity_chunk)\n","eval_novelty = np.mean(eval_novelty_chunk)\n","\n","# Print results\n","print(\n","    \"MAP:\\t\\t%f\" % eval_map,\n","    \"NDCG:\\t\\t%f\" % eval_ndcg,\n","    \"Precision@K:\\t%f\" % eval_precision,\n","    \"Recall@K:\\t%f\" % eval_recall,\n","    sep=\"\\n\",\n",")\n","\n","print(\"----\")\n","\n","print(\n","    \"Diversity:\\t\\t%f\" % eval_diversity,\n","    \"Novelty:\\t\\t%f\" % eval_novelty,\n","    \"Serendipity:\\t\\t%f\" % eval_serendipity,\n","    sep=\"\\n\",\n",")"],"metadata":{"id":"yF4i3LBemxoq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LightGCN"],"metadata":{"id":"_KmMIAo0zIU9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjwpZpdG4ORg"},"outputs":[],"source":["data = ImplicitCF(train= impli_train, test=test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2npf1Ldb4ORg","outputId":"5f6c3ffe-419d-4575-c4bf-4eca7090db3d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724426843482,"user_tz":-420,"elapsed":6959,"user":{"displayName":"16 1v1i1v8","userId":"07830316725673552979"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Already create adjacency matrix.\n","Already normalize adjacency matrix.\n","Using xavier initialization.\n"]}],"source":["model = LightGCN(hparams, data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-ItPnVW4ORh"},"outputs":[],"source":["with Timer() as train_time:\n","    model.fit()\n","\n","print(\"Took {} seconds for training.\".format(train_time.interval))"]},{"cell_type":"code","source":["# Get the unique userIDs\n","unique_user_ids = train['userID'].unique()\n","\n","# Split into chunks of 100 unique userIDs\n","user_chunks = [unique_user_ids[i:i + 200] for i in range(0, len(unique_user_ids), 200)]\n","\n","# Create a list to store the resulting DataFrames\n","train_chunks = []\n","test_chunks = []\n","\n","# Loop through each chunk and filter the DataFrame\n","for chunk in user_chunks:\n","    train_chunks.append(train[train['userID'].isin(chunk)])\n","    test_chunks.append(test[test['userID'].isin(chunk)])\n","\n","\n","eval_map_chunk = []\n","eval_ndcg_chunk = []\n","eval_precision_chunk = []\n","eval_recall_chunk = []\n","\n","eval_serendipity_chunk = []\n","eval_diversity_chunk = []\n","eval_novelty_chunk = []\n","\n","n = len(user_chunks)\n","items = train.itemID.unique()\n","i = 0\n","\n","for i in range(0,n):\n","    users = user_chunks[i]\n","    # print(users)\n","    user_data = train_chunks[i]\n","    # print(user_data)\n","    test_data = test_chunks[i]\n","\n","    all_predictions_chunk = model.recommend_k_items(test_data, top_k=TOP_K, remove_seen=True)\n","\n","\n","    eval_map_chunk.append(map_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                    relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_ndcg_chunk.append(ndcg_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    # print(eval)\n","    eval_precision_chunk.append(precision_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                      relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","    eval_recall_chunk.append(recall_at_k(test_data, all_predictions_chunk, col_prediction=\"prediction\",\n","                          relevancy_method='by_threshold', threshold=threshold, k=TOP_K))\n","\n","    rec_df = get_top_k_items(all_predictions_chunk, col_rating='prediction', k=TOP_K)\n","    eval_serendipity_chunk.append(serendipity(user_data, rec_df))\n","    eval_diversity_chunk.append(diversity(user_data, rec_df))\n","    eval_novelty_chunk.append(novelty(user_data, rec_df))\n","\n","\n","    i += 1\n","    print(i)\n","\n","\n","eval_map = np.mean(eval_map_chunk)\n","eval_ndcg = np.mean(eval_ndcg_chunk)\n","eval_precision = np.mean(eval_precision_chunk)\n","eval_recall = np.mean(eval_recall_chunk)\n","\n","eval_serendipity = np.mean(eval_serendipity_chunk)\n","eval_diversity = np.mean(eval_diversity_chunk)\n","eval_novelty = np.mean(eval_novelty_chunk)\n","\n","# Print results\n","print(\n","    \"MAP:\\t\\t%f\" % eval_map,\n","    \"NDCG:\\t\\t%f\" % eval_ndcg,\n","    \"Precision@K:\\t%f\" % eval_precision,\n","    \"Recall@K:\\t%f\" % eval_recall,\n","    sep=\"\\n\",\n",")\n","\n","print(\"----\")\n","\n","print(\n","    \"Diversity:\\t\\t%f\" % eval_diversity,\n","    \"Novelty:\\t\\t%f\" % eval_novelty,\n","    \"Serendipity:\\t\\t%f\" % eval_serendipity,\n","    sep=\"\\n\",\n",")"],"metadata":{"id":"86X5-Lqxm8Y6"},"execution_count":null,"outputs":[]}],"metadata":{"celltoolbar":"Tags","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}